# -*- coding: utf-8 -*-
"""Smoke_Removal_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mdmSJ-g8dc6aJiqcwt4NbHlJMPfFmk2

#Imports
"""

import numpy as np
import cv2
import tensorflow as tf
import keras
import matplotlib.pyplot as plt
import os

"""#Data"""

from PIL import Image
def load_hazy_dataset(directory, image_size=(256, 256)):
    print(directory + ': ')
    image_files = sorted([f for f in os.listdir(directory) if f.endswith('.png') or
                                                        f.endswith('.jpg') or
                                                        f.endswith('.jpeg')])
    images = []

    for i, filename in enumerate(image_files):
      if "_8_" in filename or "_9_" in filename or "_10_" in filename:
        image_path = os.path.join(directory, filename)
        image = Image.open(image_path)
        image = image.resize(image_size)
        images.append(np.array(image) / 255.0)
        # print("Loaded image ", i)

    return np.array(images)

from PIL import Image
def load_clear_dataset(directory, image_size=(256, 256)):
    print(directory + ': ')
    image_files = sorted([f for f in os.listdir(directory) if f.endswith('.png') or
                                                        f.endswith('.jpg') or
                                                        f.endswith('.jpeg')])
    images = []

    for i, filename in enumerate(image_files):
        image_path = os.path.join(directory, filename)
        image = Image.open(image_path)
        image = image.resize(image_size)
        images.append(np.array(image) / 255.0)
        images.append(np.array(image) / 255.0)
        images.append(np.array(image) / 255.0) #three times per image
        # print("Loaded image ", i)

    return np.array(images)

hazy_folder_path = '/SMOKE/train/hazy/'
hazy_images = load_hazy_dataset(hazy_folder_path)
print(hazy_images.shape)
print(hazy_images[0])

clear_folder_path = '/SMOKE/train/clear/'
clear_images = load_clear_dataset(clear_folder_path)
print(clear_images.shape)
print(clear_images[0])

# Considering the size of the dataset, data augmentation is probably unneccessary
# and would take a lot of computing resources

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(hazy_images, clear_images, test_size=0.2, random_state=42, shuffle=True)

"""#Model Training"""

# hyperparameters
NUM_EPOCHS = 30
BATCH_SIZE = 16
INPUT_SHAPE = (256, 256, 3)
TRIAL = 9   # MODIFY THIS BEFORE EVERY RUN
MODEL_SAVE_PATH = f'saved_models/dehazer_trial_{TRIAL}.keras'
HISTORY_SAVE_PATH = f'saved_histories/dehazer_history_trial{TRIAL}.pkl'

from tensorflow.keras import layers, models

def build_dehazing_model(input_shape=INPUT_SHAPE):
    inputs = tf.keras.Input(shape=input_shape)

    #Encoder
    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)  # Added dropout with 0.5 probability
    x = layers.MaxPooling2D((2, 2))(x)

    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)  # Added dropout with 0.5 probability

    #Decoder
    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)

    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)

    #Output layer (3 channels for RGB)
    outputs = layers.Conv2D(3, (3, 3), padding='same', activation='sigmoid')(x)

    model = models.Model(inputs, outputs, name="Dehazer")
    return model

model = build_dehazing_model()
model.summary()

# callbacks
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

checkpoint_callback = ModelCheckpoint(
    filepath=MODEL_SAVE_PATH,
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=["mse"], metrics=["mae"])

history = model.fit(X_train,
                      y_train,
                      epochs=NUM_EPOCHS,
                      batch_size=BATCH_SIZE,
                      validation_split=0.2,
                      validation_batch_size=BATCH_SIZE)

model.save(MODEL_SAVE_PATH)

import pickle
with open(HISTORY_SAVE_PATH, 'wb') as file:
  pickle.dump(history.history, file)